---
layout: default
title: 2024_하계모각코
has_children: true
permalink: /2024_code/mgk/2024_하계모각코
---
2024_하계 모각코를 진행하였다.

1회차: 2024-07-14

2회차: 2024-07-21

Binary Cross-Entropy (BCE) Loss(BCE 손실함수)

머신 러닝 및 최적화 에서 손실 함수를 정의하는 데 사용할 수 있습니다.

BCELoss
크로스 엔트로피 손실 함수는 정보 이론에서 크로스 엔트로피 개념을 기계 학습에 적용한 것이다. 
이 함수는 두 확률 분포 간의 차이를 측정하는 방법으로, BCE 손실 함수는 크로스 엔트로피 손실 함수를 이진 분류 문제에 적용한 형태이다.

\[ H(P, Q) = -\sum_{x} P(x) \log Q(x) \]

BCELoss는 모델의 구조 상에 마지막 Layer가 Sigmoid 혹은 Softmax로 되어 있는 경우 이를 사용한다. 즉, 모델의 출력이 각 라벨에 대한 확률값으로 구성되었을 때 사용이 가능하다. 
'''
torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')
'''

'''
import torch
import torch.nn as nn
m = nn.Sigmoid()
loss = nn.BCELoss()
input = torch.randn(3, 2, requires_grad=True)
target = torch.rand(3, 2)
output = loss(m(input), target)
output.backward()
'''





BCEWithLogitsLoss 
'''
torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)
'''

BCEWithLogitsLoss는 이름에서도 유추해볼 수 있듯 BCELoss를 위 과정에서 확률값(Logits)으로 변환하지 않더라도 계산되는 것을 의미한다. 
기본적인 BCE 손실 함수는 모델의 출력이 시그모이드 함수를 통과한 확률 값이어야 한다. 
그러나 이 경우수치적 불안정성(시그모이드 함수의 출력은 0과 1 사이의 값이기 때문에, 극단적인 값(예: 매우 큰 음수나 양수)에 대해 수치적으로 불안정할 수 있다.)
효율성 문제(시그모이드 함수와 BCE 손실 함수를 따로 적용하면 계산 비용이 증가할 수 있다.)가 존재할 수 있다


따라서 BCEWithLogitsLoss는 시그모이드 활성화 함수를 적용한 후 BCE 손실을 계산하는 과정을 하나의 함수로 처리한다. 
내부적으로 시그모이드 함수를 적용하고 BCE 손실을 계산하므로, 더 안정적이고 효율적이다.

\[ \text{BCEWithLogitsLoss} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(z_i)) + (1 - y_i) \log(1 - \sigma(z_i)) \right] \]

여기서:
- \( N \)은 샘플의 수
- \( y_i \)는 실제 레이블 (0 또는 1)
- \( z_i \)는 모델의 출력(로그 확률)
- \( \sigma(z) \)는 시그모이드 함수로, \(\sigma(z) = \frac{1}{1 + e^{-z}} \)





CrossEntropyLoss
'''
torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)
'''
weight:

타입: Tensor, 선택 사항
설명: 각 클래스에 대한 가중치를 지정할 수 있습니다. 클래스 불균형 문제를 해결하기 위해 사용됩니다. None으로 설정하면 모든 클래스에 동일한 가중치가 적용됩니다.
size_average:

타입: bool, 선택 사항 (기본값: None)
설명: 이 파라미터는 더 이상 사용되지 않습니다. reduction 파라미터로 대체되었습니다. True로 설정하면 손실이 평균화되고, False로 설정하면 합산됩니다.
ignore_index:

타입: int, 선택 사항
설명: 특정 클래스 인덱스를 무시할 수 있습니다. 주로 시퀀스 모델링에서 패딩 토큰을 무시하는 데 사용됩니다. 기본값은 -100입니다.
reduce:

타입: bool, 선택 사항 (기본값: None)
설명: 이 파라미터도 더 이상 사용되지 않습니다. reduction 파라미터로 대체되었습니다. True로 설정하면 손실이 축소되고, False로 설정하면 축소되지 않습니다.
reduction:

타입: str, 선택 사항
설명: 손실 결과를 어떻게 축소할지를 정의합니다. 세 가지 옵션이 있습니다:
'none': 손실을 축소하지 않고 각 샘플에 대한 손실을 반환합니다.
'mean': 손실을 평균화합니다. (size_average=True와 동일)
'sum': 손실을 합산합니다. (reduce=False와 동일)
label_smoothing:

타입: float, 선택 사항
설명: 라벨 스무딩을 적용합니다. label_smoothing 값을 [0, 1] 사이로 설정하면, 라벨을 약간의 확률로 스무딩합니다. 이는 모델이 과도하게 확신하는 것을 방지하고 일반화 성능을 향상시킬 수 있습니다. 기본값은 0.0입니다.

이전에 다룬 BCELoss와 BCEWithLogitsLoss는 Binary Classification을 위한 손실 함수다. 반면에 CrossEntropyLoss는 다중 분류를 위한 손실 함수다. 
예를 들어, 라벨이 5개라고 한다면 입력은 각 라벨에 대한 확률값을 표현하고, 정답 라벨은 라벨 값 혹은 라벨에 대한 확률값으로 표현할 수 있다. 
소프트맥스 활성화 함수와 크로스 엔트로피 손실을 결합한 함수로 예측 값과 실제 값 간의 차이를 직관적으로 표현할 수 있지만,
확률이 매우 작은 경우, 로그 함수로 인해 수치적 불안정성이 발생할 수 있고, 클래스가 불균형한 경우 성능이 저하될 수 있습니다. 이 문제를 해결하기 위해 가중치 조정 등을 사용할 수 있다.

\[ \text{CrossEntropyLoss} = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{ic} \log(p_{ic}) \]

- \( N \)은 샘플의 수
- \( C \)는 클래스의 수
- \( y_{ic} \)는 실제 레이블의 원-핫 인코딩 (실제 레이블이 \( c \) 클래스일 때 1, 그렇지 않으면 0)
- \( p_{ic} \)는 모델이 샘플 \( i \)에 대해 클래스 \( c \)일 확률로 예측한 값 (소프트맥스 함수의 출력)


3회차: 2024-08-01


Binary Cross-Entropy (BCE) Loss의 수식을 수학적으로 설명하겠습니다. BCE Loss는 이진 분류 문제에서 모델의 예측 확률과 실제 라벨 간의 차이를 측정하는 손실 함수입니다. 이를 통해 모델의 성능을 평가하고, 최적화할 수 있습니다.

BCE Loss: 

\[ L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right] \]

- \( L \)은 손실 함수의 값
- \( N \)은 총 샘플 수
- \( y_i \)는 i번째 샘플의 실제 라벨(이진 분류 문제에서는 0 또는 1)
- \( p_i \)는 i번째 샘플이 클래스 1일 확률로 모델이 예측한 값 (0과 1 사이의 값)


1. **로그항 (\( \log(p_i) \)와 \( \log(1 - p_i) \))**:
   - 로그는 정보 이론에서 엔트로피를 계산할 때 사용됩니다. 여기서 로그를 사용하는 이유는 예측 확률이 낮을 때 페널티를 크게 주기 위함입니다.
   - \( \log(p_i) \): 모델이 클래스 1일 확률을 예측한 값에 로그를 취한다.
   - \( \log(1 - p_i) \): 모델이 클래스 0일 확률을 예측한 값에 로그를 취합니다.
   
\( p_i \)가 1에 가까워질수록 \( \log(p_i) \)는 0에 가까워지고, \( p_i \)가 0에 가까워질수록 \( \log(p_i) \)는 음의 무한대로 커진다.
로그 함수를 사용하면 예측 확률이 낮을 때 손실 값이 급격히 커지게 된다. 이는 모델이 잘못된 예측을 할 경우 큰 페널티를 부과하여, 모델이 더 정확한 예측을 하도록 유도한다.


2. **실제 라벨 (\( y_i \))에 따른 조건부 손실**:
   - \( y_i = 1 \)일 때, 손실 항목은 \( \log(p_i) \)가 된다. 이는 모델이 클래스 1일 확률을 얼마나 잘 예측했는지를 나타낸다.
   - \( y_i = 0 \)일 때, 손실 항목은 \( \log(1 - p_i) \)가 된다. 이는 모델이 클래스 0일 확률을 얼마나 잘 예측했는지를 나타낸다.


3. **전체 손실 계산**:
   - \( -\left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right] \)는 i번째 샘플의 손실을 계산한다.
   - 각 샘플의 손실을 모두 더한 후, \( N \)으로 나누어 평균 손실을 구한다. 이는 샘플 수에 관계없이 일관된 손실 값을 제공한다.


4. **부호**:
   - 확률p가 0.5보다 작으면 로그 값은 음수가 되기 때문에 해석하기 어려워질 수 있고, 해석이 일관되게 하기 위해서, BCE Loss는 로그 값의 음수를 취하여 손실 값을 양수로 만든다. \( 손실 값을 양수로 만들기 위해 전체에 -1을 곱합니다.\)


 BCE Loss는 모델이 예측한 확률 \( p_i \)와 실제 라벨 \( y_i \) 간의 차이를 로그 함수와 결합하여 측정하며, 이를 통해 모델이 얼마나 잘 예측하는지를 평가한다. 이 손실 함수를 최소화함으로써 모델의 성능을 최적화할 수 있다.

